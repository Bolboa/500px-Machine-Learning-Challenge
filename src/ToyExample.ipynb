{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1> Toy Example for Breaking Linear Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process for breaking linear classifiers is most easily understood if we take the simplest example that describes the problem. In this example, we take a binary logistic regression and have it map the input to one of two classes, which can be either class 1 or class 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we procure our input and the weights we will use to map the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "# training set\n",
    "x = numpy.array([2, -1, 3, -2, 2, 2, 1, -4, 5, 1])\n",
    "# weights\n",
    "w = numpy.array([-1, -1, 1, -1, 1, -1, 1, 1, -1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, we do the dot product of the input and the weights and we run this through a sigmoid function to get the probability of the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dot product\n",
    "prod = numpy.dot(x, w)\n",
    "# get the classifier probability\n",
    "probability(sigmoid(prod))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of the *probability* function is to tell us whether the class of the input is either class 1 or class 0, and tells us how confident the classifier isin its prediction. It was found that the result of the dot product **-3**, and applying the sigmoid function to this, the result is **0.0474**. Correspondingly,  the probability function prints out the follwing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import math\n",
    "\n",
    "# sigmoid function\n",
    "def sigmoid(z):\n",
    "    return 1/(1 + math.exp(-z))\n",
    "\n",
    "# classifier probability\n",
    "def probability(sig_val):\n",
    "    if sig_val < 0.5:\n",
    "        result = \"Classifier is {}% certain that value is class 0\".format(round((1-sig_val)*100, 2))\n",
    "        print(result)\n",
    "    else:\n",
    "        result = \"Classifier is {}% certain that value is class 1\".format(round(sig_val*100, 2))\n",
    "        print(result)\n",
    "\n",
    "\n",
    "def main():\n",
    "    # training set\n",
    "    x = numpy.array([2, -1, 3, -2, 2, 2, 1, -4, 5, 1])\n",
    "    # weights\n",
    "    w = numpy.array([-1, -1, 1, -1, 1, -1, 1, 1, -1, 1])\n",
    "\n",
    "    # dot product\n",
    "    prod = numpy.dot(x, w)\n",
    "    # get the classifier probability\n",
    "    probability(sigmoid(prod))\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to break the classifier, we need to shift the values of the input in a certain direction by the samllest amount possible so as to get the classifier to predict the wrong class. The reason we want to shift by a small amount is because we want to leave the input as seemingly untouched as possible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, imagine if the input was the pixel coordinates of an image. In that case, we want to change the values of the coordinates by such an amount so that the image looks the same to the naked eye, but still tricks the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that the dot product is the directional growth of one vector to another. So, in order to increase the result of the dot product we need to increase this directional growth. Where the weight is positive, we increase the corresponding input by 0.5, and when the weight is negative we decrease it by the same amount. Our new input look as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "# training set\n",
    "x = numpy.array([2, -1, 3, -2, 2, 2, 1, -4, 5, 1])\n",
    "# weights\n",
    "w = numpy.array([-1, -1, 1, -1, 1, -1, 1, 1, -1, 1])\n",
    "\n",
    "# increase training data by a fraction of each weight\n",
    "xad = x + 0.5*w\n",
    "\n",
    "print(xad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dot product is now **2**, which is greater than the previous result of **-3**, and we only changed each input value by a small amount. Applying sigmoid to this result yields the value **0.88**. Now, if we run the following code, the probability function will predict something entirely different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import math\n",
    "\n",
    "# sigmoid function\n",
    "def sigmoid(z):\n",
    "    return 1/(1 + math.exp(-z))\n",
    "\n",
    "# classifier probability\n",
    "def probability(sig_val):\n",
    "    if sig_val < 0.5:\n",
    "        result = \"Classifier is {}% certain that value is class 0\".format(round((1-sig_val)*100, 2))\n",
    "        print(result)\n",
    "    else:\n",
    "        result = \"Classifier is {}% certain that value is class 1\".format(round(sig_val*100, 2))\n",
    "        print(result)\n",
    "\n",
    "\n",
    "def main():\n",
    "    # training set\n",
    "    x = numpy.array([2, -1, 3, -2, 2, 2, 1, -4, 5, 1])\n",
    "    # weights\n",
    "    w = numpy.array([-1, -1, 1, -1, 1, -1, 1, 1, -1, 1])\n",
    "\n",
    "    # increase training data by a fraction of each weight\n",
    "    xad = x + 0.5*w\n",
    "\n",
    "    # get the new classifier probability\n",
    "    prod = numpy.dot(xad, w)\n",
    "    probability(sigmoid(prod))\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}